{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Método para tratar o dado que tenha valor tanto para teste quanto para treino.\n",
    "2. Método para submission preparation.\n",
    "3. método para gerar arquivo de submissão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "import ast\n",
    "import dtale\n",
    "import random\n",
    "\n",
    "from sklearn import preprocessing as pp\n",
    "import pickle\n",
    "\n",
    "from sklearn import model_selection as ms\n",
    "import category_encoders as ce\n",
    "\n",
    "from boruta   import BorutaPy\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble        import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from xgboost                 import XGBRegressor\n",
    "from sklearn.linear_model    import LogisticRegression, SGDRegressor\n",
    "from sklearn.neighbors       import KNeighborsRegressor\n",
    "from sklearn.tree            import DecisionTreeRegressor\n",
    "from sklearn.ensemble        import RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.metrics         import accuracy_score, recall_score, precision_score, balanced_accuracy_score, f1_score, roc_curve, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.naive_bayes     import GaussianNB\n",
    "from lightgbm                import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Function Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(url):\n",
    "    # Opening JSON file\n",
    "    train = open(url)\n",
    "\n",
    "    # returns JSON object as a dictionary\n",
    "    data_train_aux = json.load(train)\n",
    "\n",
    "    data_train = pd.json_normalize(data_train_aux, record_path = 'data')\n",
    "    data_train.columns = data_train_aux['columns']\n",
    "    return data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpeza_inicial(df):\n",
    "    \n",
    "    if 'actual_price' in df.columns:\n",
    "        df = df.drop(columns=['description', 'images','crawled_at','title'])\n",
    "        prices = df[['seller','pid', 'actual_price']].groupby('pid').max().reset_index()\n",
    "    \n",
    "        precos_nulos = df.loc[df['actual_price'].isna(), ['_id','pid']]\n",
    "\n",
    "        precos_recuperados = pd.merge(precos_nulos, prices, how = 'inner', on = 'pid')\n",
    "\n",
    "        df_aux = pd.merge(df, precos_recuperados[['_id', 'actual_price']], on = '_id', how = 'left')\n",
    "        df['actual_price'] = df_aux['actual_price_x'].fillna(0) + df_aux['actual_price_y'].fillna(0)\n",
    "\n",
    "        df = df.dropna(subset=['actual_price'])\n",
    "        \n",
    "    else:\n",
    "        df = df.drop(columns=['description', 'images','crawled_at','title'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    try:\n",
    "        return ast.literal_eval(str(x))   \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "def junta_dict(dict_list):\n",
    "        dicionario = {}\n",
    "        for d in dict_list:\n",
    "            for key, value in d.items():\n",
    "                dicionario[key] = value\n",
    "        return dicionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_product_detais (df):\n",
    "    df_pd = df['product_details'].apply(lambda x: f(x))\n",
    "    \n",
    "    df_pd = pd.DataFrame([junta_dict(row) for row in df_pd], index = df_pd.index)\n",
    "    \n",
    "    df_pd = df_pd.drop(columns=['', ' '])\n",
    "\n",
    "    # Removendo valores faltantes acima de 50%\n",
    "    limite_nulos = len(df_pd) * 0.5  \n",
    "    df_pd = df_pd.dropna(thresh = limite_nulos, axis=1)\n",
    "    \n",
    "    return df_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_pd_plus_df(df, df_pd):\n",
    "    df = pd.concat([df, df_pd], axis=1)\n",
    "    df = df.drop( ['product_details'] , axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ajuste_colunas (df):\n",
    "    \n",
    "    df.loc[df['Fabric'].isnull(), 'Fabric'] = 'outros'\n",
    "    df.loc[df['Pattern'].isnull(), 'Pattern'] = 'outros'\n",
    "    df.loc[df['Style Code'].isnull(), 'Style Code'] = 'outros'\n",
    "    df.loc[df['Pack of'].isnull(), 'Pack of'] = 'outros'\n",
    "    df.loc[df['Type'].isnull(), 'Type'] = 'outros'\n",
    "    df.loc[df['Sleeve'].isnull(), 'Sleeve'] = 'outros'\n",
    "    df.loc[df['Fit'].isnull(), 'Fit'] = 'outros'\n",
    "    df.loc[df['Ideal For'].isnull(), 'Ideal For'] = 'outros'\n",
    "    df.loc[df['Suitable For'].isnull(), 'Suitable For'] = 'outros'\n",
    "    df.loc[df['Reversible'].isnull(), 'Reversible'] = 'outros'\n",
    "    df.loc[df['Fabric Care'].isnull(), 'Fabric Care'] = 'outros'\n",
    "    \n",
    "    # # Suponha que você tenha uma coluna chamada 'Fabric' com valores do tipo object\n",
    "    # df_new['Fabric'] = df_new['Fabric'].astype(str)\n",
    "    # df_new['Pattern'] = df_new['Pattern'].astype(str)\n",
    "    # df_new['Style Code'] = df_new['Style Code'].astype(str)\n",
    "    # df_new['Pack of'] = df_new['Pack of'].astype(str)\n",
    "    # df_new['Type'] = df_new['Type'].astype(str)\n",
    "    # df_new['Sleeve'] = df_new['Sleeve'].astype(str)\n",
    "    # df_new['Fit'] = df_new['Fit'].astype(str)\n",
    "    # df_new['Ideal For'] = df_new['Ideal For'].astype(str)\n",
    "    # df_new['Suitable For'] = df_new['Suitable For'].astype(str)\n",
    "    # df_new['Reversible'] = df_new['Reversible'].astype(str)\n",
    "    # df_new['Fabric Care'] = df_new['Fabric Care'].astype(str)\n",
    "\n",
    "\n",
    "    # # Agora você pode usar o método str.split para dividir os valores\n",
    "    # df_new['Fabric'] = df_new['Fabric'].str.split(',')\n",
    "    # df_new['Pattern'] = df_new['Pattern'].str.split(',')\n",
    "    # df_new['Style Code'] = df_new['Style Code'].str.split(',')\n",
    "    # df_new['Pack of'] = df_new['Pack of'].str.split(',')\n",
    "    # df_new['Type'] = df_new['Type'].str.split(',')\n",
    "    # df_new['Sleeve'] = df_new['Sleeve'].str.split(',')\n",
    "    # df_new['Fit'] = df_new['Fit'].str.split(',')\n",
    "    # df_new['Ideal For'] = df_new['Ideal For'].str.split(',')\n",
    "    # df_new['Suitable For'] = df_new['Suitable For'].str.split(',')\n",
    "    # df_new['Reversible'] = df_new['Reversible'].str.split(',')\n",
    "    # df_new['Fabric Care'] = df_new['Fabric Care'].str.split(',')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_sex(df):\n",
    "\n",
    "# Dados sem virgulas: Style Code, Pack of, Suitable For, Reversible, 'Ideal For'\n",
    "\n",
    "\n",
    "# Ideal for: 'Men', 'Men, Boys'/ 'Boys, Men' = 'all_ages',\n",
    "            # 'Boys, Girls, Men, Women' = 'unisex_all_ages', 'Women, Men' = 'Unisex'\n",
    "    \n",
    "    df['Ideal For'] = df['Ideal For'].apply(lambda x : 'men' if x == 'Men'\n",
    "                                            else 'all_ages' if x == 'Men, Boys'\n",
    "                                            else 'all_ages' if x == 'Boys, Men'\n",
    "                                            else 'unisex_all_ages' if x == 'unisex_all_ages'\n",
    "                                            else 'unisex' if x == 'Women, Men'\n",
    "                                            else 'outros')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engine (df):\n",
    "    \n",
    "    ## out_of_stock - transformar para int \n",
    "    df['out_of_stock'] = df['out_of_stock'].astype('int64')\n",
    "\n",
    "    # brand - substituir por outros\n",
    "    df['brand'] = df['brand'].apply(lambda x: x.lower())\n",
    "\n",
    "    # criando feature product\n",
    "    df['product'] = df[['category', 'sub_category']].apply(lambda x: x['category'] + '_' + x['sub_category'], axis = 1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df(df):\n",
    "    \n",
    "    if 'actual_price' in df.columns:\n",
    "        X = df.drop(columns=['_id','pid','actual_price'], axis=1)\n",
    "        y = df['actual_price']\n",
    "        X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.1)\n",
    "        data_train = pd.concat([X_train, y_train], axis=1)\n",
    "        data_test = pd.concat([X_test, y_test], axis=1)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        X_prod = df.drop(columns=['_id','pid'], axis=1)\n",
    "        return X_prod \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_resposta(y_train):\n",
    "    # Tranformando a variável resposta em logaritmica\n",
    "    y_train = np.log1p(y_train)\n",
    "    return y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescaling_train(df_train):\n",
    "    \n",
    "    # 'avg_delivery_time_days',\n",
    "    mm = pp.MinMaxScaler()\n",
    "\n",
    "    df_train['avg_delivery_time_days'] = mm.fit_transform(df_train[['avg_delivery_time_days']])\n",
    "    pickle.dump(mm, open('parameters/mm_avg_delivery_time_days.pkl','wb'))\n",
    "\n",
    "    rs_average_rating = pp.RobustScaler()\n",
    "    rs_number_of_reviews = pp.RobustScaler()\n",
    "\n",
    "    df_train['average_rating'] = rs_average_rating.fit_transform(df_train[['average_rating']])\n",
    "    pickle.dump(rs_average_rating, open('parameters/rs_average_rating.pkl','wb'))\n",
    "\n",
    "    df_train['number_of_reviews'] = rs_number_of_reviews.fit_transform(df_train[['number_of_reviews']])\n",
    "    pickle.dump(rs_number_of_reviews, open('parameters/rs_number_of_reviews.pkl','wb'))\n",
    "    \n",
    "    return df_train\n",
    "\n",
    "    ##### Fazer Robust Scaler da variável resposta (??) #####\n",
    "\n",
    "def rescaling_test(df_test):\n",
    "    \n",
    "    mm = pickle.load(open('parameters/mm_avg_delivery_time_days.pkl','rb'))\n",
    "    df_test['avg_delivery_time_days'] = mm.transform(df_test[['avg_delivery_time_days']])\n",
    "    \n",
    "    rs_ar = pickle.load(open('parameters/rs_average_rating.pkl','rb'))\n",
    "    df_test['average_rating'] = rs_ar.transform(df_test[['average_rating']])\n",
    "    \n",
    "    rs_nr = pickle.load(open('parameters/rs_number_of_reviews.pkl','rb'))\n",
    "    df_test['number_of_reviews'] = rs_nr.transform(df_test[['number_of_reviews']])\n",
    "    \n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranform_train(X_train, y_train):\n",
    "    \n",
    "    #FREQUENCY ENCODER\n",
    "    for att in ['brand',\n",
    "                'product',\n",
    "                'seller',\n",
    "                'category',\n",
    "                'Suitable For',\n",
    "                'Reversible',\n",
    "                'Ideal For',\n",
    "                'out_of_stock',\n",
    "                'Style Code',\n",
    "                'Pack of']:\n",
    "        fe =  ce.CountEncoder(cols=[att], normalize=True).fit(X_train)\n",
    "        pickle.dump(fe,open(f'parameters/fe_{att}.pkl', 'wb'))\n",
    "        X_train = fe.transform(X_train)\n",
    "    \n",
    "    # Instancie o codificador alvo para 'Style Code'\n",
    "    #te_style_code = ce.TargetEncoder()\n",
    "    #te_pack_of = ce.TargetEncoder()\n",
    "    \n",
    "    #X_train['Style Code'] = te_style_code.fit_transform(X_train['Style Code'], y_train) \n",
    "    #pickle.dump(te_style_code, open('parameters/te_style_code.pkl', 'wb'))\n",
    "\n",
    "    # Instancie o codificador alvo para 'Pack of'\n",
    "    #X_train['Pack of'] = te_pack_of.fit_transform(X_train['Pack of'], y_train) \n",
    "    #pickle.dump(te_pack_of, open('parameters/te_pack_of.pkl', 'wb'))\n",
    "    \n",
    "    return X_train\n",
    "\n",
    "def tranform_test(X_test):\n",
    "    \n",
    "    # FREQUENCY ENCODER\n",
    "    for att in ['brand',\n",
    "                'product',\n",
    "                'seller',\n",
    "                'category',\n",
    "                'Suitable For',\n",
    "                'Reversible',\n",
    "                'Ideal For',\n",
    "                'out_of_stock',\n",
    "                'Style Code',\n",
    "                'Pack of']:\n",
    "        fe = pickle.load(open(f'parameters/fe_{att}.pkl', 'rb'))\n",
    "        X_test = fe.transform(X_test)\n",
    "\n",
    "    # TARGET ENCODE\n",
    "    #te_style_code = pickle.load(open('parameters/te_style_code.pkl','rb'))\n",
    "    #X_test['Style Code'] = te_style_code.transform(X_test['Style Code'])\n",
    "\n",
    "    #te_pack_of = pickle.load(open('parameters/te_pack_of.pkl','rb'))\n",
    "    #X_test['Pack of'] = te_pack_of.transform(X_test['Pack of'])\n",
    "\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_select_LGBM (X_train, y_train):\n",
    "    \n",
    "    # Crie e treine o modelo LGBMClassifier\n",
    "    lgb_model = LGBMRegressor(n_jobs=-1, random_state=42)\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    # Obtenha as importâncias das características do modelo\n",
    "    importances = lgb_model.feature_importances_\n",
    "\n",
    "    # Ordene as características por importância decrescente\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Imprima o ranking das características\n",
    "    print('Feature ranking')\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for i, j in zip(X_train.columns, importances):\n",
    "        aux = pd.DataFrame({'feature': i, 'importance': j}, index=[0])\n",
    "\n",
    "        df = pd.concat([df, aux], axis=0)\n",
    "        df = df.sort_values('importance', ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selected_BORUTA(X_train, y_train):\n",
    "    rf = RandomForestRegressor()\n",
    "    \n",
    "    # Define Boruta - Essa peste demora 1 hora ou mais.\n",
    "    boruta = BorutaPy( rf, n_estimators = 'auto', verbose=2, random_state=42).fit( X_train, y_train)\n",
    "\n",
    "    cols_selected = boruta.support_.tolist()\n",
    "    print(cols_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selected_manual (df):\n",
    "    features_selected = ['average_rating',\n",
    "    #'number_of_reviews',\n",
    "    'brand',\n",
    "    # 'category',\n",
    "    # 'crawled_at',\n",
    "    'out_of_stock',\n",
    "    'avg_delivery_time_days',\n",
    "    #  'product_details',\n",
    "    'seller',\n",
    "    #  'sub_category',\n",
    "    #  'fabrication_time',\n",
    "    #  'title',\n",
    "    #  'Fabric',\n",
    "    #  'Pattern',\n",
    "    'Style Code',\n",
    "    'Pack of',\n",
    "    #  'Type',\n",
    "    #  'Sleeve',\n",
    "    #  'Fit',\n",
    "    'Ideal For',\n",
    "    'Suitable For',\n",
    "    'Reversible',\n",
    "    #  'Fabric Care',\n",
    "    'product']\n",
    "    \n",
    "    return df[features_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MachineLearning(X_train, y_train, X_test, y_test):\n",
    "    SEED = 42\n",
    "\n",
    "    lista_de_medidas = ['SMAPE']\n",
    "    nome_das_medidas = ['SMAPE']\n",
    "\n",
    "    lista_de_modelos = [XGBRegressor(learning_rate=0.1, n_estimators=100, max_depth=3),\n",
    "                        SGDRegressor(random_state=SEED), \n",
    "                        LGBMRegressor(random_state=SEED, n_jobs=-1, force_row_wise=True, ),\n",
    "                        DecisionTreeRegressor(random_state=SEED),\n",
    "                        KNeighborsRegressor(n_neighbors=5,  weights='distance',n_jobs=-1),\n",
    "                        BaggingRegressor(),\n",
    "                        RandomForestRegressor(random_state=SEED)]\n",
    "\n",
    "    nome_dos_modelos = ['XGBoost', \n",
    "                        'SGD', \n",
    "                        'LGBM', \n",
    "                        'DecisionTree',\n",
    "                        'KNN',\n",
    "                        'Bagging',\n",
    "                        'RandomForest']\n",
    "\n",
    "    resultados0 = {}\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    for i in range(len(lista_de_modelos)):\n",
    "        print('Rodando modelo: ' + nome_dos_modelos[i])\n",
    "        \n",
    "        model = lista_de_modelos[i]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        smape = 100 / len(y_test) * np.sum(2 * np.abs(np.expm1(y_pred) - np.expm1(y_test)) / (np.abs(np.expm1(y_test)) + np.abs(np.expm1(y_pred))))\n",
    "        # Calculo meigarom: 100 / len(y_test) * np.sum(2 * np.abs(y_pred - y_test) / (np.abs(y_test) + np.abs(y_pred)))\n",
    "\n",
    "        resultados0[nome_dos_modelos[i]] = [smape]\n",
    "    \n",
    "    resultados = pd.DataFrame(resultados0, index = nome_das_medidas).T\n",
    "\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation (model_name, X_training, kfold, model, verbose = False ):\n",
    "    smape_list = []\n",
    "    \n",
    "    \n",
    "    # Star an end date validation\n",
    "    for k in reversed( range (1, kfold) ):\n",
    "        if verbose:\n",
    "            print('\\nKfold: {}'.format(k))\n",
    "        validation_model_start = (len(X_training)/(kfold-k))\n",
    "        validation_model_end =  (len(X_training)) - (validation_model_start)\n",
    "\n",
    "        # Filter Data Set\n",
    "        treino = X_training.head(validation_model_start)\n",
    "        validacao = X_training.tail(validation_model_end)\n",
    "\n",
    "        # Training and validation dataset\n",
    "        # Treino\n",
    "        xtreino = treino.drop( ['actual_price'] , axis = 1)\n",
    "        ytreino = treino['actual_price']\n",
    "\n",
    "        # Validação\n",
    "        xvalidacao = validacao.drop( ['actual_price'] , axis = 1)\n",
    "        yvalidacao = validacao['actual_price']\n",
    "\n",
    "        # Modelo\n",
    "        m = model.fit( xtreino, ytreino )\n",
    "\n",
    "        # Predição\n",
    "        yhat_m = m.predict( xvalidacao )\n",
    "        \n",
    "        smape = 100 / len(yvalidacao) * np.sum(2 * np.abs(np.expm1(yhat_m) - np.expm1(yvalidacao)) / (np.abs(np.expm1(yvalidacao)) + np.abs(np.expm1(yhat_m))))\n",
    "        # Calculo meigarom: 100 / len(y_test) * np.sum(2 * np.abs(y_pred - y_test) / (np.abs(y_test) + np.abs(y_pred)))\n",
    "        \n",
    "        # Guardando as performaces\n",
    "        smape_list.append( smape )\n",
    "        \n",
    "        \n",
    "    resposta_modelo = pd.DataFrame( {'Model Name': model_name,\n",
    "                                     'SMAPE': smape_list},\n",
    "                                   index = [0])\n",
    "    \n",
    "    # Fazendo somente por questões de aprendizagem\n",
    "    #curiosidade = pd.DataFrame({'Iteracao' : iteracao,\n",
    "    #                            'Treino Início' : treino_inicio,\n",
    "    #                            'Treino Fim' : treino_fim,\n",
    "    #                            'Validação Início' : validação_inicio,\n",
    "    #                            'Validação Fim' : validação_fim} )\n",
    "    return resposta_modelo #, curiosidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective_cv(trial):  \n",
    "    \n",
    "#     n_estimators = trial.suggest_int('n_estimators',20, 100)\n",
    "#     eta = trial.suggest_float('eta',0.01, 0.05)\n",
    "#     max_depth = trial.suggest_int('max_depth', 3, 9)\n",
    "#     subsample = trial.suggest_float('subsample',0.1, 0.9)\n",
    "#     colsample_bytree = trial.suggest_float('colsample_bytree',0.1,0.9)\n",
    "\n",
    "#     # model definition\n",
    "#     xgb_model = XGBRegressor( n_estimators    = n_estimators, \n",
    "#                               eta              = eta, \n",
    "#                               max_depth        = max_depth, \n",
    "#                               subsample        = subsample,\n",
    "#                               colsample_bytree = colsample_bytree\n",
    "#                              )\n",
    "    \n",
    "#     xgb_model.fit(X_train[features_selected], y_train)\n",
    "    \n",
    "#     # Model Prediction \n",
    "#     y_pred = xgb_model.predict(X_test_mod[features_selected])\n",
    "\n",
    "#     # calculando rmse\n",
    "#     smape = 100 / len(y_test_mod) * np.sum(2 * np.abs(np.expm1(y_pred) - np.expm1(y_test_mod)) / (np.abs(np.expm1(y_test_mod)) + np.abs(np.expm1(y_pred))))\n",
    "\n",
    "#     return smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective_cv, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tunnung (X_train, y_train): \n",
    "    #RANDON SEARCH\n",
    "    param = {'n_estimators' : [1500, 1700, 3000, 3500],\n",
    "         'eta' : [0.01, 0.03],\n",
    "         'max_depth' : [3, 5, 9],\n",
    "         'subsample': [0.1, 0.5, 0.7],\n",
    "         'colsample_bytree' : [0.3, 0.7, 0.9],\n",
    "         'min_child_weight' : [3, 8, 15]}\n",
    "\n",
    "    MAX_EVAL = 10\n",
    "    \n",
    "    final_result = pd.DataFrame()\n",
    "    for i in range (MAX_EVAL):\n",
    "        # Escolher os parâmetros aleatórios\n",
    "        hp = { k: random.sample(v, 1)[0] for k, v in param.items() }\n",
    "        print(hp)\n",
    "\n",
    "\n",
    "        # model\n",
    "        model_xgb = XGBRegressor(objective = 'reg:squarederror',\n",
    "                                n_estimators=hp['n_estimators'],\n",
    "                                eta = hp['eta'],\n",
    "                                max_depth = hp['max_depth'],\n",
    "                                subsample = hp['subsample'],\n",
    "                                colsample_bytree = hp['colsample_bytree'],\n",
    "                                min_child_weight = hp['min_child_weight'])\n",
    "        # performance\n",
    "\n",
    "        result = cross_validation (model_name = 'XGBRegressor',\n",
    "                                   X_training = X_train,\n",
    "                                   kfold = 5,\n",
    "                                   model = model_xgb,\n",
    "                                   verbose = True )\n",
    "        final_result = pd.concat([final_result, result])\n",
    "    final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratamento_inicial (df):\n",
    "    \n",
    "    df = limpeza_inicial (df)\n",
    "    df_pd = expand_product_detais (df)\n",
    "    df = df_pd_plus_df(df, df_pd)\n",
    "    df = ajuste_colunas (df)\n",
    "    df = type_sex(df)\n",
    "    df = feature_engine (df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model(model, x_test):\n",
    "    \n",
    "    yhat = np.expm1(model.predict(x_test))\n",
    "    \n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Function Subission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(df_sub,yhat):\n",
    "    # Preparando os dados para a submissão\n",
    "\n",
    "    df_submission = pd.DataFrame()\n",
    "    df_submission['pid'] = df_sub['pid']\n",
    "    df_submission['actual_price'] = yhat\n",
    "\n",
    "    # Exportando como CSV\n",
    "    df_submission.to_csv('submissions/submission_g01.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_treino = '../2023-10-20-Hackday_6/dataset/raw/train.json'\n",
    "url_prod = '../2023-10-20-Hackday_6/dataset/raw/test.json'\n",
    "\n",
    "df_treino = load_data(url_treino)\n",
    "df_prod = load_data(url_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Run Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treino = tratamento_inicial(df_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prod = tratamento_inicial(df_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_df(df_treino)\n",
    "y_train = var_resposta(y_train)\n",
    "y_test = var_resposta(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prod = split_df(df_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = rescaling_train(X_train)\n",
    "X_test = rescaling_test(X_test)\n",
    "X_prod = rescaling_test(X_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tranform_train(X_train, y_train)\n",
    "X_test = tranform_test(X_test)\n",
    "X_prod = tranform_test(X_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = feature_selected_manual (X_train)\n",
    "X_test = feature_selected_manual (X_test)\n",
    "X_prod = feature_selected_manual (X_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002354 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 592\n",
      "[LightGBM] [Info] Number of data points in the train set: 18647, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 6.152875\n",
      "Feature ranking\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Style Code</td>\n",
       "      <td>964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>average_rating</td>\n",
       "      <td>646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seller</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brand</td>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>product</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pack of</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>out_of_stock</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>avg_delivery_time_days</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>number_of_reviews</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  feature  importance\n",
       "0              Style Code         964\n",
       "0          average_rating         646\n",
       "0                  seller         503\n",
       "0                   brand         457\n",
       "0                 product         252\n",
       "0                 Pack of         148\n",
       "0            out_of_stock          18\n",
       "0  avg_delivery_time_days          12\n",
       "0       number_of_reviews           0"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetures = feature_select_LGBM (X_train, y_train)\n",
    "fetures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_selected_BORUTA(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Ideal For', 'Suitable For', 'Reversible'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[607], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_selected_manual\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m X_test \u001b[38;5;241m=\u001b[39m feature_selected_manual (X_test)\n\u001b[0;32m      3\u001b[0m X_prod \u001b[38;5;241m=\u001b[39m feature_selected_manual (X_prod)\n",
      "Cell \u001b[1;32mIn[606], line 27\u001b[0m, in \u001b[0;36mfeature_selected_manual\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_selected_manual\u001b[39m (df):\n\u001b[0;32m      2\u001b[0m     features_selected \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_rating\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m#'number_of_reviews',\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrand\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m#  'Fabric Care',\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures_selected\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32md:\\Git_Hub\\Competicoes\\CDS\\HackDays\\2023-10-20-Hackday_6\\hd6\\lib\\site-packages\\pandas\\core\\frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3812\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 3813\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   3815\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[1;32md:\\Git_Hub\\Competicoes\\CDS\\HackDays\\2023-10-20-Hackday_6\\hd6\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6067\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6070\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6072\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   6073\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6074\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32md:\\Git_Hub\\Competicoes\\CDS\\HackDays\\2023-10-20-Hackday_6\\hd6\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6133\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6132\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 6133\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Ideal For', 'Suitable For', 'Reversible'] not in index\""
     ]
    }
   ],
   "source": [
    "X_train = feature_selected_manual (X_train)\n",
    "X_test = feature_selected_manual (X_test)\n",
    "X_prod = feature_selected_manual (X_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rodando modelo: XGBoost\n",
      "Rodando modelo: SGD\n",
      "Rodando modelo: LGBM\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Total Bins 403\n",
      "[LightGBM] [Info] Number of data points in the train set: 20978, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 6.154553\n",
      "Rodando modelo: DecisionTree\n",
      "Rodando modelo: KNN\n",
      "Rodando modelo: Bagging\n",
      "Rodando modelo: RandomForest\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>104.651421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD</th>\n",
       "      <td>104.491846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM</th>\n",
       "      <td>104.648291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTree</th>\n",
       "      <td>96.443726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>93.833053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging</th>\n",
       "      <td>99.172782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>99.360827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   SMAPE\n",
       "XGBoost       104.651421\n",
       "SGD           104.491846\n",
       "LGBM          104.648291\n",
       "DecisionTree   96.443726\n",
       "KNN            93.833053\n",
       "Bagging        99.172782\n",
       "RandomForest   99.360827"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado = MachineLearning(X_train, y_train, X_test, y_test)\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine_tunnung (X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Submission Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'n_estimators': 100,\n",
    " 'eta': 0.04847968659990019,\n",
    " 'max_depth': 7,\n",
    " 'subsample': 0.31355832178198495,\n",
    " 'colsample_bytree': 0.5456941887941139}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = XGBRegressor(n_jobs = -1 , \n",
    "                                 random_state=0,\n",
    "                                 n_estimators = best_params['n_estimators'],\n",
    "                                 eta = best_params['eta'],\n",
    "                                 max_depth = best_params['max_depth'],\n",
    "                                 subsample = best_params['subsample'],\n",
    "                                 colsample_bytree = best_params['colsample_bytree'])\n",
    "\n",
    "# model fit\n",
    "best_model.fit(X_train[features_selected], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = best_model(best_model, X_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission(df_prod,yhat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('hd6': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "892e5ff5783333d733f0bd2e0068297de9409e96214f7e68af4469f351f76fae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
