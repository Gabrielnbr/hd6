{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Método para tratar o dado que tenha valor tanto para teste quanto para treino.\n",
    "2. Método para submission preparation.\n",
    "3. método para gerar arquivo de submissão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "import ast\n",
    "import dtale\n",
    "import random\n",
    "\n",
    "from sklearn import preprocessing as pp\n",
    "import pickle\n",
    "\n",
    "from sklearn import model_selection as ms\n",
    "import category_encoders as ce\n",
    "\n",
    "from boruta   import BorutaPy\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble        import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from xgboost                 import XGBRegressor\n",
    "from sklearn.linear_model    import LogisticRegression, SGDRegressor\n",
    "from sklearn.neighbors       import KNeighborsRegressor\n",
    "from sklearn.tree            import DecisionTreeRegressor\n",
    "from sklearn.ensemble        import RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.metrics         import accuracy_score, recall_score, precision_score, balanced_accuracy_score, f1_score, roc_curve, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.naive_bayes     import GaussianNB\n",
    "from lightgbm                import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Function Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(url):\n",
    "    # Opening JSON file\n",
    "    train = open(url)\n",
    "\n",
    "    # returns JSON object as a dictionary\n",
    "    data_train_aux = json.load(train)\n",
    "\n",
    "    data_train = pd.json_normalize(data_train_aux, record_path = 'data')\n",
    "    data_train.columns = data_train_aux['columns']\n",
    "    return data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpeza_inicial(df):\n",
    "    \n",
    "    if 'actual_price' in df.columns:\n",
    "        df = df.drop(columns=['description', 'images','crawled_at','title'])\n",
    "        prices = df[['seller','pid', 'actual_price']].groupby('pid').max().reset_index()\n",
    "    \n",
    "        precos_nulos = df.loc[df['actual_price'].isna(), ['_id','pid']]\n",
    "\n",
    "        precos_recuperados = pd.merge(precos_nulos, prices, how = 'inner', on = 'pid')\n",
    "\n",
    "        df_aux = pd.merge(df, precos_recuperados[['_id', 'actual_price']], on = '_id', how = 'left')\n",
    "        df['actual_price'] = df_aux['actual_price_x'].fillna(0) + df_aux['actual_price_y'].fillna(0)\n",
    "\n",
    "        df = df.dropna(subset=['actual_price'])\n",
    "        \n",
    "    else:\n",
    "        df = df.drop(columns=['description', 'images','crawled_at','title'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    try:\n",
    "        return ast.literal_eval(str(x))   \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "def junta_dict(dict_list):\n",
    "        dicionario = {}\n",
    "        for d in dict_list:\n",
    "            for key, value in d.items():\n",
    "                dicionario[key] = value\n",
    "        return dicionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_product_detais (df):\n",
    "    df_pd = df['product_details'].apply(lambda x: f(x))\n",
    "    \n",
    "    df_pd = pd.DataFrame([junta_dict(row) for row in df_pd], index = df_pd.index)\n",
    "    \n",
    "    df_pd = df_pd.drop(columns=['', ' '])\n",
    "\n",
    "    # Removendo valores faltantes acima de 50%\n",
    "    limite_nulos = len(df_pd) * 0.5  \n",
    "    df_pd = df_pd.dropna(thresh = limite_nulos, axis=1)\n",
    "    \n",
    "    return df_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_pd_plus_df(df, df_pd):\n",
    "    df = pd.concat([df, df_pd], axis=1)\n",
    "    df = df.drop( ['product_details'] , axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ajuste_colunas (df):\n",
    "    \n",
    "    df.loc[df['Fabric'].isnull(), 'Fabric'] = 'outros'\n",
    "    df.loc[df['Pattern'].isnull(), 'Pattern'] = 'outros'\n",
    "    df.loc[df['Style Code'].isnull(), 'Style Code'] = 'outros'\n",
    "    df.loc[df['Pack of'].isnull(), 'Pack of'] = 'outros'\n",
    "    df.loc[df['Type'].isnull(), 'Type'] = 'outros'\n",
    "    df.loc[df['Sleeve'].isnull(), 'Sleeve'] = 'outros'\n",
    "    df.loc[df['Fit'].isnull(), 'Fit'] = 'outros'\n",
    "    df.loc[df['Ideal For'].isnull(), 'Ideal For'] = 'outros'\n",
    "    df.loc[df['Suitable For'].isnull(), 'Suitable For'] = 'outros'\n",
    "    df.loc[df['Reversible'].isnull(), 'Reversible'] = 'outros'\n",
    "    df.loc[df['Fabric Care'].isnull(), 'Fabric Care'] = 'outros'\n",
    "    \n",
    "    # # Suponha que você tenha uma coluna chamada 'Fabric' com valores do tipo object\n",
    "    # df_new['Fabric'] = df_new['Fabric'].astype(str)\n",
    "    # df_new['Pattern'] = df_new['Pattern'].astype(str)\n",
    "    # df_new['Style Code'] = df_new['Style Code'].astype(str)\n",
    "    # df_new['Pack of'] = df_new['Pack of'].astype(str)\n",
    "    # df_new['Type'] = df_new['Type'].astype(str)\n",
    "    # df_new['Sleeve'] = df_new['Sleeve'].astype(str)\n",
    "    # df_new['Fit'] = df_new['Fit'].astype(str)\n",
    "    # df_new['Ideal For'] = df_new['Ideal For'].astype(str)\n",
    "    # df_new['Suitable For'] = df_new['Suitable For'].astype(str)\n",
    "    # df_new['Reversible'] = df_new['Reversible'].astype(str)\n",
    "    # df_new['Fabric Care'] = df_new['Fabric Care'].astype(str)\n",
    "\n",
    "\n",
    "    # # Agora você pode usar o método str.split para dividir os valores\n",
    "    # df_new['Fabric'] = df_new['Fabric'].str.split(',')\n",
    "    # df_new['Pattern'] = df_new['Pattern'].str.split(',')\n",
    "    # df_new['Style Code'] = df_new['Style Code'].str.split(',')\n",
    "    # df_new['Pack of'] = df_new['Pack of'].str.split(',')\n",
    "    # df_new['Type'] = df_new['Type'].str.split(',')\n",
    "    # df_new['Sleeve'] = df_new['Sleeve'].str.split(',')\n",
    "    # df_new['Fit'] = df_new['Fit'].str.split(',')\n",
    "    # df_new['Ideal For'] = df_new['Ideal For'].str.split(',')\n",
    "    # df_new['Suitable For'] = df_new['Suitable For'].str.split(',')\n",
    "    # df_new['Reversible'] = df_new['Reversible'].str.split(',')\n",
    "    # df_new['Fabric Care'] = df_new['Fabric Care'].str.split(',')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_sex(df):\n",
    "\n",
    "# Dados sem virgulas: Style Code, Pack of, Suitable For, Reversible, 'Ideal For'\n",
    "\n",
    "\n",
    "# Ideal for: 'Men', 'Men, Boys'/ 'Boys, Men' = 'all_ages',\n",
    "            # 'Boys, Girls, Men, Women' = 'unisex_all_ages', 'Women, Men' = 'Unisex'\n",
    "    \n",
    "    df['Ideal For'] = df['Ideal For'].apply(lambda x : 'men' if x == 'Men'\n",
    "                                            else 'all_ages' if x == 'Men, Boys'\n",
    "                                            else 'all_ages' if x == 'Boys, Men'\n",
    "                                            else 'unisex_all_ages' if x == 'unisex_all_ages'\n",
    "                                            else 'unisex' if x == 'Women, Men'\n",
    "                                            else 'outros')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engine (df):\n",
    "    \n",
    "    ## out_of_stock - transformar para int \n",
    "    df['out_of_stock'] = df['out_of_stock'].astype('int64')\n",
    "\n",
    "    # brand - substituir por outros\n",
    "    df['brand'] = df['brand'].apply(lambda x: x.lower())\n",
    "\n",
    "    # criando feature product\n",
    "    df['product'] = df[['category', 'sub_category']].apply(lambda x: x['category'] + '_' + x['sub_category'], axis = 1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df(df):\n",
    "    \n",
    "    if 'actual_price' in df.columns:\n",
    "        X = df.drop(columns=['_id','pid','actual_price'], axis=1)\n",
    "        y = df['actual_price']\n",
    "        X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.20)\n",
    "        data_train = pd.concat([X_train, y_train], axis=1)\n",
    "        data_test = pd.concat([X_test, y_test], axis=1)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        X_prod = df.drop(columns=['_id','pid'], axis=1)\n",
    "        return X_prod \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_resposta(y_train):\n",
    "    # Tranformando a variável resposta em logaritmica\n",
    "    y_train = np.log1p(y_train)\n",
    "    return y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescaling_train(df_train):\n",
    "    \n",
    "    # 'avg_delivery_time_days',\n",
    "    mm = pp.MinMaxScaler()\n",
    "\n",
    "    df_train['avg_delivery_time_days'] = mm.fit_transform(df_train[['avg_delivery_time_days']])\n",
    "    pickle.dump(mm, open('parameters/mm_avg_delivery_time_days.pkl','wb'))\n",
    "\n",
    "    rs_average_rating = pp.RobustScaler()\n",
    "    rs_number_of_reviews = pp.RobustScaler()\n",
    "\n",
    "    df_train['average_rating'] = rs_average_rating.fit_transform(df_train[['average_rating']])\n",
    "    pickle.dump(rs_average_rating, open('parameters/rs_average_rating.pkl','wb'))\n",
    "\n",
    "    df_train['number_of_reviews'] = rs_number_of_reviews.fit_transform(df_train[['number_of_reviews']])\n",
    "    pickle.dump(rs_number_of_reviews, open('parameters/rs_number_of_reviews.pkl','wb'))\n",
    "    \n",
    "    return df_train\n",
    "\n",
    "    ##### Fazer Robust Scaler da variável resposta (??) #####\n",
    "\n",
    "def rescaling_test(df_test):\n",
    "    \n",
    "    mm = pickle.load(open('parameters/mm_avg_delivery_time_days.pkl','rb'))\n",
    "    df_test['avg_delivery_time_days'] = mm.transform(df_test[['avg_delivery_time_days']])\n",
    "    \n",
    "    rs_ar = pickle.load(open('parameters/rs_average_rating.pkl','rb'))\n",
    "    df_test['average_rating'] = rs_ar.transform(df_test[['average_rating']])\n",
    "    \n",
    "    rs_nr = pickle.load(open('parameters/rs_number_of_reviews.pkl','rb'))\n",
    "    df_test['number_of_reviews'] = rs_nr.transform(df_test[['number_of_reviews']])\n",
    "    \n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranform_train(X_train, y_train):\n",
    "    \n",
    "    #FREQUENCY ENCODER\n",
    "    for att in ['brand',\n",
    "                'product',\n",
    "                'seller',\n",
    "                'category',\n",
    "                'Suitable For',\n",
    "                'Reversible',\n",
    "                'Ideal For',\n",
    "                'out_of_stock']:\n",
    "        fe =  ce.CountEncoder(cols=[att], normalize=True).fit(X_train)\n",
    "        pickle.dump(fe,open(f'parameters/fe_{att}.pkl', 'wb'))\n",
    "        X_train = fe.transform(X_train)\n",
    "    \n",
    "    # Instancie o codificador alvo para 'Style Code'\n",
    "    te_style_code = ce.TargetEncoder()\n",
    "    te_pack_of = ce.TargetEncoder()\n",
    "    \n",
    "    X_train['Style Code'] = te_style_code.fit_transform(X_train['Style Code'], y_train) \n",
    "    pickle.dump(te_style_code, open('parameters/te_style_code.pkl', 'wb'))\n",
    "\n",
    "    # Instancie o codificador alvo para 'Pack of'\n",
    "    X_train['Pack of'] = te_pack_of.fit_transform(X_train['Pack of'], y_train) \n",
    "    pickle.dump(te_pack_of, open('parameters/te_pack_of.pkl', 'wb'))\n",
    "    \n",
    "    return X_train\n",
    "\n",
    "def tranform_test(X_test):\n",
    "    \n",
    "    # FREQUENCY ENCODER\n",
    "    for att in ['brand',\n",
    "                'product',\n",
    "                'seller',\n",
    "                'category',\n",
    "                'Suitable For',\n",
    "                'Reversible',\n",
    "                'Ideal For',\n",
    "                'out_of_stock']:\n",
    "        fe = pickle.load(open(f'parameters/fe_{att}.pkl', 'rb'))\n",
    "        X_test = fe.transform(X_test)\n",
    "\n",
    "    # TARGET ENCODE\n",
    "    te_style_code = pickle.load(open('parameters/te_style_code.pkl','rb'))\n",
    "    X_test['Style Code'] = te_style_code.transform(X_test['Style Code'])\n",
    "\n",
    "    te_pack_of = pickle.load(open('parameters/te_pack_of.pkl','rb'))\n",
    "    X_test['Pack of'] = te_pack_of.transform(X_test['Pack of'])\n",
    "\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_select_LGBM (X_train, y_train):\n",
    "    \n",
    "    # Crie e treine o modelo LGBMClassifier\n",
    "    lgb_model = LGBMRegressor(n_jobs=-1, random_state=42)\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    # Obtenha as importâncias das características do modelo\n",
    "    importances = lgb_model.feature_importances_\n",
    "\n",
    "    # Ordene as características por importância decrescente\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Imprima o ranking das características\n",
    "    print('Feature ranking')\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for i, j in zip(X_train.columns, importances):\n",
    "        aux = pd.DataFrame({'feature': i, 'importance': j}, index=[0])\n",
    "\n",
    "        df = pd.concat([df, aux], axis=0)\n",
    "        df = df.sort_values('importance', ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selected_BORUTA(X_train, y_train):\n",
    "    rf = RandomForestRegressor()\n",
    "    \n",
    "    # Define Boruta - Essa peste demora 1 hora ou mais.\n",
    "    boruta = BorutaPy( rf, n_estimators = 'auto', verbose=2, random_state=42).fit( X_train, y_train)\n",
    "\n",
    "    cols_selected = boruta.support_.tolist()\n",
    "    print(cols_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selected_manual (df):\n",
    "    features_selected = ['average_rating',\n",
    "    'number_of_reviews',\n",
    "    'brand',\n",
    "    # 'category',\n",
    "    # 'crawled_at',\n",
    "    'out_of_stock',\n",
    "    'avg_delivery_time_days',\n",
    "    #  'product_details',\n",
    "    'seller',\n",
    "    #  'sub_category',\n",
    "    #  'fabrication_time',\n",
    "    #  'title',\n",
    "    #  'Fabric',\n",
    "    #  'Pattern',\n",
    "    'Style Code',\n",
    "    'Pack of',\n",
    "    #  'Type',\n",
    "    #  'Sleeve',\n",
    "    #  'Fit',\n",
    "    'Ideal For',\n",
    "    'Suitable For',\n",
    "    'Reversible',\n",
    "    #  'Fabric Care',\n",
    "    'product']\n",
    "    \n",
    "    return df[features_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MachineLearning(X_train, y_train, X_test, y_test):\n",
    "    SEED = 42\n",
    "\n",
    "    lista_de_medidas = ['SMAPE']\n",
    "    nome_das_medidas = ['SMAPE']\n",
    "\n",
    "    lista_de_modelos = [XGBRegressor(learning_rate=0.1, n_estimators=100, max_depth=3),\n",
    "                        SGDRegressor(random_state=SEED), \n",
    "                        LGBMRegressor(random_state=SEED, n_jobs=-1, force_row_wise=True, ),\n",
    "                        DecisionTreeRegressor(random_state=SEED),\n",
    "                        KNeighborsRegressor(n_neighbors=5,  weights='distance',n_jobs=-1),\n",
    "                        BaggingRegressor(),\n",
    "                        RandomForestRegressor(random_state=SEED)]\n",
    "\n",
    "    nome_dos_modelos = ['XGBoost', \n",
    "                        'SGD', \n",
    "                        'LGBM', \n",
    "                        'DecisionTree',\n",
    "                        'KNN',\n",
    "                        'Bagging',\n",
    "                        'RandomForest']\n",
    "\n",
    "    resultados0 = {}\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    for i in range(len(lista_de_modelos)):\n",
    "        print('Rodando modelo: ' + nome_dos_modelos[i])\n",
    "        \n",
    "        model = lista_de_modelos[i]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        smape = 100 / len(y_test) * np.sum(2 * np.abs(np.expm1(y_pred) - np.expm1(y_test)) / (np.abs(np.expm1(y_test)) + np.abs(np.expm1(y_pred))))\n",
    "        # Calculo meigarom: 100 / len(y_test) * np.sum(2 * np.abs(y_pred - y_test) / (np.abs(y_test) + np.abs(y_pred)))\n",
    "\n",
    "        resultados0[nome_dos_modelos[i]] = [smape]\n",
    "    \n",
    "    resultados = pd.DataFrame(resultados0, index = nome_das_medidas).T\n",
    "\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation (model_name, X_training, kfold, model, verbose = False ):\n",
    "    smape_list = []\n",
    "    \n",
    "    \n",
    "    # Star an end date validation\n",
    "    for k in reversed( range (1, kfold) ):\n",
    "        if verbose:\n",
    "            print('\\nKfold: {}'.format(k))\n",
    "        validation_model_start = (len(X_training)/(kfold-k))\n",
    "        validation_model_end =  (len(X_training)) - (validation_model_start)\n",
    "\n",
    "        # Filter Data Set\n",
    "        treino = X_training.head(validation_model_start)\n",
    "        validacao = X_training.tail(validation_model_end)\n",
    "\n",
    "        # Training and validation dataset\n",
    "        # Treino\n",
    "        xtreino = treino.drop( ['actual_price'] , axis = 1)\n",
    "        ytreino = treino['actual_price']\n",
    "\n",
    "        # Validação\n",
    "        xvalidacao = validacao.drop( ['actual_price'] , axis = 1)\n",
    "        yvalidacao = validacao['actual_price']\n",
    "\n",
    "        # Modelo\n",
    "        m = model.fit( xtreino, ytreino )\n",
    "\n",
    "        # Predição\n",
    "        yhat_m = m.predict( xvalidacao )\n",
    "        \n",
    "        smape = 100 / len(yvalidacao) * np.sum(2 * np.abs(np.expm1(yhat_m) - np.expm1(yvalidacao)) / (np.abs(np.expm1(yvalidacao)) + np.abs(np.expm1(yhat_m))))\n",
    "        # Calculo meigarom: 100 / len(y_test) * np.sum(2 * np.abs(y_pred - y_test) / (np.abs(y_test) + np.abs(y_pred)))\n",
    "        \n",
    "        # Guardando as performaces\n",
    "        smape_list.append( smape )\n",
    "        \n",
    "        \n",
    "    resposta_modelo = pd.DataFrame( {'Model Name': model_name,\n",
    "                                     'SMAPE': smape_list},\n",
    "                                   index = [0])\n",
    "    \n",
    "    # Fazendo somente por questões de aprendizagem\n",
    "    #curiosidade = pd.DataFrame({'Iteracao' : iteracao,\n",
    "    #                            'Treino Início' : treino_inicio,\n",
    "    #                            'Treino Fim' : treino_fim,\n",
    "    #                            'Validação Início' : validação_inicio,\n",
    "    #                            'Validação Fim' : validação_fim} )\n",
    "    return resposta_modelo #, curiosidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tunnung (X_train, y_train): \n",
    "    #RANDON SEARCH\n",
    "    param = {'n_estimators' : [1500, 1700, 3000, 3500],\n",
    "         'eta' : [0.01, 0.03],\n",
    "         'max_depth' : [3, 5, 9],\n",
    "         'subsample': [0.1, 0.5, 0.7],\n",
    "         'colsample_bytree' : [0.3, 0.7, 0.9],\n",
    "         'min_child_weight' : [3, 8, 15]}\n",
    "\n",
    "    MAX_EVAL = 10\n",
    "    \n",
    "    final_result = pd.DataFrame()\n",
    "    for i in range (MAX_EVAL):\n",
    "        # Escolher os parâmetros aleatórios\n",
    "        hp = { k: random.sample(v, 1)[0] for k, v in param.items() }\n",
    "        print(hp)\n",
    "\n",
    "\n",
    "        # model\n",
    "        model_xgb = XGBRegressor(objective = 'reg:squarederror',\n",
    "                                n_estimators=hp['n_estimators'],\n",
    "                                eta = hp['eta'],\n",
    "                                max_depth = hp['max_depth'],\n",
    "                                subsample = hp['subsample'],\n",
    "                                colsample_bytree = hp['colsample_bytree'],\n",
    "                                min_child_weight = hp['min_child_weight'])\n",
    "        # performance\n",
    "\n",
    "        result = cross_validation (model_name = 'XGBRegressor',\n",
    "                                   X_training = X_train,\n",
    "                                   kfold = 5,\n",
    "                                   model = model_xgb,\n",
    "                                   verbose = True )\n",
    "        final_result = pd.concat([final_result, result])\n",
    "    final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratamento_inicial (df):\n",
    "    \n",
    "    df = limpeza_inicial (df)\n",
    "    df_pd = expand_product_detais (df)\n",
    "    df = df_pd_plus_df(df, df_pd)\n",
    "    df = ajuste_colunas (df)\n",
    "    df = type_sex(df)\n",
    "    df = feature_engine (df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model(model, x_test):\n",
    "    \n",
    "    yhat = np.expm1(model.predict(x_test))\n",
    "    \n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Function Subission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(df_sub,yhat):\n",
    "    # Preparando os dados para a submissão\n",
    "\n",
    "    df_submission = pd.DataFrame()\n",
    "    df_submission['pid'] = df_sub['pid']\n",
    "    df_submission['actual_price'] = yhat\n",
    "\n",
    "    # Exportando como CSV\n",
    "    df_submission.to_csv('submissions/submission_g01.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_treino = '../2023-10-20-Hackday_6/dataset/raw/train.json'\n",
    "url_prod = '../2023-10-20-Hackday_6/dataset/raw/test.json'\n",
    "\n",
    "df_treino = load_data(url_treino)\n",
    "df_prod = load_data(url_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Run Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treino = tratamento_inicial(df_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prod = tratamento_inicial(df_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_df(df_treino)\n",
    "y_train = var_resposta(y_train)\n",
    "y_test = var_resposta(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prod = split_df(df_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = rescaling_train(X_train)\n",
    "X_test = rescaling_test(X_test)\n",
    "X_prod = rescaling_test(X_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tranform_train(X_train, y_train)\n",
    "X_test = tranform_test(X_test)\n",
    "X_prod = tranform_test(X_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = feature_selected_manual (X_train)\n",
    "X_test = feature_selected_manual (X_test)\n",
    "X_prod = feature_selected_manual (X_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002885 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 608\n",
      "[LightGBM] [Info] Number of data points in the train set: 18647, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 6.149542\n",
      "Feature ranking\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Style Code</td>\n",
       "      <td>871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>average_rating</td>\n",
       "      <td>643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seller</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brand</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>product</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pack of</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ideal For</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Suitable For</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reversible</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>out_of_stock</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>avg_delivery_time_days</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>number_of_reviews</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  feature  importance\n",
       "0              Style Code         871\n",
       "0          average_rating         643\n",
       "0                  seller         468\n",
       "0                   brand         394\n",
       "0                 product         257\n",
       "0                 Pack of         142\n",
       "0               Ideal For          97\n",
       "0            Suitable For          54\n",
       "0              Reversible          47\n",
       "0            out_of_stock          18\n",
       "0  avg_delivery_time_days           9\n",
       "0       number_of_reviews           0"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetures = feature_select_LGBM (X_train, y_train)\n",
    "fetures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_selected_BORUTA(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = feature_selected_manual (X_train)\n",
    "X_test = feature_selected_manual (X_test)\n",
    "X_prod = feature_selected_manual (X_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rodando modelo: XGBoost\n",
      "Rodando modelo: SGD\n",
      "Rodando modelo: LGBM\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Total Bins 608\n",
      "[LightGBM] [Info] Number of data points in the train set: 18647, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 6.149542\n",
      "Rodando modelo: DecisionTree\n",
      "Rodando modelo: KNN\n",
      "Rodando modelo: Bagging\n",
      "Rodando modelo: RandomForest\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>96.289011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD</th>\n",
       "      <td>103.976491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM</th>\n",
       "      <td>90.773819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTree</th>\n",
       "      <td>87.702980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>78.280850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging</th>\n",
       "      <td>91.189373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>92.370933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   SMAPE\n",
       "XGBoost        96.289011\n",
       "SGD           103.976491\n",
       "LGBM           90.773819\n",
       "DecisionTree   87.702980\n",
       "KNN            78.280850\n",
       "Bagging        91.189373\n",
       "RandomForest   92.370933"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado = MachineLearning(X_train, y_train, X_test, y_test)\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine_tunnung (X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Submission Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsRegressor(n_neighbors=5,  weights='distance',n_jobs=-1)\n",
    "model_train = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = best_model(model_train, X_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission(df_prod,yhat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('hd6': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "892e5ff5783333d733f0bd2e0068297de9409e96214f7e68af4469f351f76fae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
