{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "import ast\n",
    "import dtale\n",
    "import random\n",
    "import optuna\n",
    "\n",
    "\n",
    "from sklearn import preprocessing as pp\n",
    "import pickle\n",
    "\n",
    "from sklearn import model_selection as ms\n",
    "import category_encoders as ce\n",
    "\n",
    "from boruta   import BorutaPy\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble        import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from xgboost                 import XGBRegressor\n",
    "from sklearn.linear_model    import LogisticRegression, SGDRegressor\n",
    "from sklearn.neighbors       import KNeighborsRegressor\n",
    "from sklearn.tree            import DecisionTreeRegressor\n",
    "from sklearn.ensemble        import RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.naive_bayes     import GaussianNB\n",
    "from lightgbm                import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Function Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(url):\n",
    "    # Opening JSON file\n",
    "    train = open(url)\n",
    "\n",
    "    # returns JSON object as a dictionary\n",
    "    data_train_aux = json.load(train)\n",
    "\n",
    "    data_train = pd.json_normalize(data_train_aux, record_path = 'data')\n",
    "    data_train.columns = data_train_aux['columns']\n",
    "    return data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpeza_inicial(df):\n",
    "    \n",
    "    if 'actual_price' in df.columns:\n",
    "        \n",
    "        df = df.loc[df['actual_price'] < 6000]\n",
    "        \n",
    "        df = df.drop(columns=['description', 'images','crawled_at','title'])\n",
    "        prices = df[['seller','pid', 'actual_price']].groupby('pid').max().reset_index()\n",
    "    \n",
    "        precos_nulos = df.loc[df['actual_price'].isna(), ['_id','pid']]\n",
    "\n",
    "        precos_recuperados = pd.merge(precos_nulos, prices, how = 'inner', on = 'pid')\n",
    "\n",
    "        df_aux = pd.merge(df, precos_recuperados[['_id', 'actual_price']], on = '_id', how = 'left')\n",
    "        df['actual_price'] = df_aux['actual_price_x'].fillna(0) + df_aux['actual_price_y'].fillna(0)\n",
    "\n",
    "        df = df.dropna(subset=['actual_price'])\n",
    "        \n",
    "    else:\n",
    "        df = df.drop(columns=['description', 'images','crawled_at','title'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    try:\n",
    "        return ast.literal_eval(str(x))   \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def junta_dict(dict_list):\n",
    "        dicionario = {}\n",
    "        for d in dict_list:\n",
    "            for key, value in d.items():\n",
    "                dicionario[key] = value\n",
    "        return dicionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_product_detais (df):\n",
    "    df_pd = df['product_details'].apply(lambda x: f(x))\n",
    "    \n",
    "    df_pd = pd.DataFrame([junta_dict(row) for row in df_pd], index = df_pd.index)\n",
    "    \n",
    "    df_pd = df_pd.drop(columns=['', ' '])\n",
    "\n",
    "    # Removendo valores faltantes acima de 50%\n",
    "    limite_nulos = len(df_pd) * 0.5  \n",
    "    df_pd = df_pd.dropna(thresh = limite_nulos, axis=1)\n",
    "    \n",
    "    return df_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_pd_plus_df(df, df_pd):\n",
    "    df = pd.concat([df, df_pd], axis=1)\n",
    "    df = df.drop( ['product_details'] , axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ajuste_colunas (df):\n",
    "    \n",
    "    df.loc[df['Fabric'].isnull(), 'Fabric'] = 'outros'\n",
    "    df.loc[df['Pattern'].isnull(), 'Pattern'] = 'outros'\n",
    "    df.loc[df['Style Code'].isnull(), 'Style Code'] = 'outros'\n",
    "    df.loc[df['Pack of'].isnull(), 'Pack of'] = 'outros'\n",
    "    df.loc[df['Type'].isnull(), 'Type'] = 'outros'\n",
    "    df.loc[df['Sleeve'].isnull(), 'Sleeve'] = 'outros'\n",
    "    df.loc[df['Fit'].isnull(), 'Fit'] = 'outros'\n",
    "    df.loc[df['Ideal For'].isnull(), 'Ideal For'] = 'outros'\n",
    "    df.loc[df['Suitable For'].isnull(), 'Suitable For'] = 'outros'\n",
    "    df.loc[df['Reversible'].isnull(), 'Reversible'] = 'outros'\n",
    "    df.loc[df['Fabric Care'].isnull(), 'Fabric Care'] = 'outros'\n",
    "    \n",
    "    # # Suponha que você tenha uma coluna chamada 'Fabric' com valores do tipo object\n",
    "    # df_new['Fabric'] = df_new['Fabric'].astype(str)\n",
    "    # df_new['Pattern'] = df_new['Pattern'].astype(str)\n",
    "    # df_new['Style Code'] = df_new['Style Code'].astype(str)\n",
    "    # df_new['Pack of'] = df_new['Pack of'].astype(str)\n",
    "    # df_new['Type'] = df_new['Type'].astype(str)\n",
    "    # df_new['Sleeve'] = df_new['Sleeve'].astype(str)\n",
    "    # df_new['Fit'] = df_new['Fit'].astype(str)\n",
    "    # df_new['Ideal For'] = df_new['Ideal For'].astype(str)\n",
    "    # df_new['Suitable For'] = df_new['Suitable For'].astype(str)\n",
    "    # df_new['Reversible'] = df_new['Reversible'].astype(str)\n",
    "    # df_new['Fabric Care'] = df_new['Fabric Care'].astype(str)\n",
    "\n",
    "\n",
    "    # # Agora você pode usar o método str.split para dividir os valores\n",
    "    # df_new['Fabric'] = df_new['Fabric'].str.split(',')\n",
    "    # df_new['Pattern'] = df_new['Pattern'].str.split(',')\n",
    "    # df_new['Style Code'] = df_new['Style Code'].str.split(',')\n",
    "    # df_new['Pack of'] = df_new['Pack of'].str.split(',')\n",
    "    # df_new['Type'] = df_new['Type'].str.split(',')\n",
    "    # df_new['Sleeve'] = df_new['Sleeve'].str.split(',')\n",
    "    # df_new['Fit'] = df_new['Fit'].str.split(',')\n",
    "    # df_new['Ideal For'] = df_new['Ideal For'].str.split(',')\n",
    "    # df_new['Suitable For'] = df_new['Suitable For'].str.split(',')\n",
    "    # df_new['Reversible'] = df_new['Reversible'].str.split(',')\n",
    "    # df_new['Fabric Care'] = df_new['Fabric Care'].str.split(',')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_sex(df):\n",
    "# Dados sem virgulas: Style Code, Pack of, Suitable For, Reversible, 'Ideal For'\n",
    "\n",
    "# Ideal for: 'Men', 'Men, Boys'/ 'Boys, Men' = 'all_ages',\n",
    "            # 'Boys, Girls, Men, Women' = 'unisex_all_ages', 'Women, Men' = 'Unisex'\n",
    "    \n",
    "    df['Ideal For'] = df['Ideal For'].apply(lambda x : 'men' if x == 'Men'\n",
    "                                            else 'all_ages' if x == 'Men, Boys'\n",
    "                                            else 'all_ages' if x == 'Boys, Men'\n",
    "                                            else 'unisex_all_ages' if x == 'unisex_all_ages'\n",
    "                                            else 'unisex' if x == 'Women, Men'\n",
    "                                            else 'outros')\n",
    "    \n",
    "    # Novas Features de roupa\n",
    "\n",
    "    df['cotton'] = df['Fabric'].apply(lambda x: 1 if 'cotton' in x.lower() else 0)\n",
    "    df['poly'] = df['Fabric'].apply(lambda x: 1 if 'poly' in x.lower() else 0)\n",
    "    df['not_cotton_or_poly'] = df['Fabric'].apply(lambda x: 1 if ('poly' not in x.lower()) and ('cotton' not in x.lower()) else 0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engine (df):\n",
    "    \n",
    "    ## out_of_stock - transformar para int \n",
    "    df['out_of_stock'] = df['out_of_stock'].astype('int64')\n",
    "\n",
    "    # brand - substituir por outros\n",
    "    df['brand'] = df['brand'].apply(lambda x: x.lower())\n",
    "\n",
    "    # criando feature product\n",
    "    df['product'] = df[['category', 'sub_category']].apply(lambda x: x['category'] + '_' + x['sub_category'], axis = 1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df(df):\n",
    "    \n",
    "    if 'actual_price' in df.columns:\n",
    "        X = df.drop(columns=['_id','pid','actual_price'], axis=1)\n",
    "        y = df['actual_price']\n",
    "        X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.20)\n",
    "        data_train = pd.concat([X_train, y_train], axis=1)\n",
    "        data_test = pd.concat([X_test, y_test], axis=1)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        X_prod = df.drop(columns=['_id','pid'], axis=1)\n",
    "        return X_prod \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_resposta(y_train):\n",
    "    # Tranformando a variável resposta em logaritmica\n",
    "    y_train = np.log1p(y_train)\n",
    "    return y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescaling_train(df_train):\n",
    "    \n",
    "    # 'avg_delivery_time_days',\n",
    "    mm = pp.MinMaxScaler()\n",
    "\n",
    "    df_train['avg_delivery_time_days'] = mm.fit_transform(df_train[['avg_delivery_time_days']])\n",
    "    pickle.dump(mm, open('parameters/mm_avg_delivery_time_days.pkl','wb'))\n",
    "\n",
    "    rs_average_rating = pp.RobustScaler()\n",
    "    rs_number_of_reviews = pp.RobustScaler()\n",
    "\n",
    "    df_train['average_rating'] = rs_average_rating.fit_transform(df_train[['average_rating']])\n",
    "    pickle.dump(rs_average_rating, open('parameters/rs_average_rating.pkl','wb'))\n",
    "\n",
    "    df_train['number_of_reviews'] = rs_number_of_reviews.fit_transform(df_train[['number_of_reviews']])\n",
    "    pickle.dump(rs_number_of_reviews, open('parameters/rs_number_of_reviews.pkl','wb'))\n",
    "    \n",
    "    return df_train\n",
    "\n",
    "    ##### Fazer Robust Scaler da variável resposta (??) #####\n",
    "\n",
    "def rescaling_test(df_test):\n",
    "    \n",
    "    mm = pickle.load(open('parameters/mm_avg_delivery_time_days.pkl','rb'))\n",
    "    df_test['avg_delivery_time_days'] = mm.transform(df_test[['avg_delivery_time_days']])\n",
    "    \n",
    "    rs_ar = pickle.load(open('parameters/rs_average_rating.pkl','rb'))\n",
    "    df_test['average_rating'] = rs_ar.transform(df_test[['average_rating']])\n",
    "    \n",
    "    rs_nr = pickle.load(open('parameters/rs_number_of_reviews.pkl','rb'))\n",
    "    df_test['number_of_reviews'] = rs_nr.transform(df_test[['number_of_reviews']])\n",
    "    \n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranform_train(X_train, y_train):\n",
    "    \n",
    "    \n",
    "#    #ONE HOT ENCODE\n",
    "#    X_train = pd.get_dummies( X_train, prefix = ['IF'], columns = ['Ideal For'] )\n",
    "#    X_train = pd.get_dummies( X_train, prefix = ['RV'], columns = ['Reversible'] )\n",
    "#    X_train = pd.get_dummies( X_train, prefix = ['SF'], columns = ['Suitable For'] )\n",
    "#    X_train = pd.get_dummies( X_train, prefix = ['PO'], columns = ['Pack of'] )\n",
    "#    X_train = pd.get_dummies( X_train, prefix = ['PT'], columns = ['Pattern'] )\n",
    "#    \n",
    "    #FREQUENCY ENCODER\n",
    "    for att in ['brand',\n",
    "                'product',\n",
    "                'seller',\n",
    "                'category',\n",
    "                'Suitable For',\n",
    "                'Reversible',\n",
    "                'Ideal For',\n",
    "                'out_of_stock',\n",
    "                'Pattern',\n",
    "                'Pack of'\n",
    "                ]:\n",
    "        fe =  ce.CountEncoder(cols=[att], normalize=True).fit(X_train)\n",
    "        pickle.dump(fe,open(f'parameters/fe_{att}.pkl', 'wb'))\n",
    "        X_train = fe.transform(X_train)\n",
    "    \n",
    "    # Instancie o codificador alvo para 'Style Code'\n",
    "    te_style_code = ce.TargetEncoder()\n",
    "    te_pack_of = ce.TargetEncoder()\n",
    "    \n",
    "    X_train['Style Code'] = te_style_code.fit_transform(X_train['Style Code'], y_train) \n",
    "    pickle.dump(te_style_code, open('parameters/te_style_code.pkl', 'wb'))\n",
    "\n",
    "    # Instancie o codificador alvo para 'Pack of'\n",
    "    X_train['Pack of'] = te_pack_of.fit_transform(X_train['Pack of'], y_train) \n",
    "    pickle.dump(te_pack_of, open('parameters/te_pack_of.pkl', 'wb'))\n",
    "    \n",
    "    return X_train\n",
    "\n",
    "def tranform_test(X_test):\n",
    "    \n",
    "    #ONE HOT ENCODE\n",
    "#    X_test = pd.get_dummies( X_test, prefix = ['IF'], columns = ['Ideal For'] )\n",
    "#    X_test = pd.get_dummies( X_test, prefix = ['RV'], columns = ['Reversible'] )\n",
    "#    X_test = pd.get_dummies( X_test, prefix = ['SF'], columns = ['Suitable For'] )\n",
    "#    X_test = pd.get_dummies( X_test, prefix = ['PO'], columns = ['Pack of'] )\n",
    "#    X_test = pd.get_dummies( X_test, prefix = ['PT'], columns = ['Pattern'] )\n",
    "    \n",
    "    # FREQUENCY ENCODER\n",
    "    for att in ['brand',\n",
    "                'product',\n",
    "                'seller',\n",
    "                'category',\n",
    "                'Suitable For',\n",
    "                'Reversible',\n",
    "                'Ideal For',\n",
    "                'out_of_stock',\n",
    "                'Pattern',\n",
    "                'Pack of'\n",
    "                ]:\n",
    "        fe = pickle.load(open(f'parameters/fe_{att}.pkl', 'rb'))\n",
    "        X_test = fe.transform(X_test)\n",
    "\n",
    "    # TARGET ENCODE\n",
    "    te_style_code = pickle.load(open('parameters/te_style_code.pkl','rb'))\n",
    "    X_test['Style Code'] = te_style_code.transform(X_test['Style Code'])\n",
    "\n",
    "    te_pack_of = pickle.load(open('parameters/te_pack_of.pkl','rb'))\n",
    "    X_test['Pack of'] = te_pack_of.transform(X_test['Pack of'])\n",
    "\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_select_LGBM (X_train, y_train):\n",
    "    \n",
    "    # Crie e treine o modelo LGBMClassifier\n",
    "    lgb_model = LGBMRegressor(n_jobs=-1, random_state=42)\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    # Obtenha as importâncias das características do modelo\n",
    "    importances = lgb_model.feature_importances_\n",
    "\n",
    "    # Ordene as características por importância decrescente\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Imprima o ranking das características\n",
    "    print('Feature ranking')\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for i, j in zip(X_train.columns, importances):\n",
    "        aux = pd.DataFrame({'feature': i, 'importance': j}, index=[0])\n",
    "\n",
    "        df = pd.concat([df, aux], axis=0)\n",
    "        df = df.sort_values('importance', ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selected_BORUTA(X_train, y_train):\n",
    "    rf = RandomForestRegressor()\n",
    "    \n",
    "    # Define Boruta - Essa peste demora 1 hora ou mais.\n",
    "    boruta = BorutaPy( rf, n_estimators = 'auto', verbose=2, random_state=42).fit( X_train, y_train)\n",
    "\n",
    "    cols_selected = boruta.support_.tolist()\n",
    "    print(cols_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selected_manual (df):\n",
    "    features_selected = [\n",
    "        'average_rating',\n",
    "        #'number_of_reviews',\n",
    "        'brand',\n",
    "        # 'category',\n",
    "        # 'crawled_at',\n",
    "        'out_of_stock',\n",
    "        #'avg_delivery_time_days',\n",
    "        #  'product_details',\n",
    "        'seller',\n",
    "        #  'sub_category',\n",
    "        #  'fabrication_time',\n",
    "        #  'title',\n",
    "        #  'Fabric',\n",
    "        #  'Pattern',\n",
    "        'cotton',\n",
    "        'poly',\n",
    "        'Style Code',\n",
    "        'Pack of',\n",
    "        #  'Type',\n",
    "        #  'Sleeve',\n",
    "        #  'Fit',\n",
    "        'Ideal For',\n",
    "        'Suitable For',\n",
    "        'Reversible',\n",
    "        #  'Fabric Care',\n",
    "        'product'\n",
    "        ]\n",
    "    return df[features_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MachineLearning(X_train, y_train, X_test, y_test):\n",
    "    SEED = 42\n",
    "\n",
    "    lista_de_medidas = ['SMAPE']\n",
    "    nome_das_medidas = ['SMAPE']\n",
    "\n",
    "    lista_de_modelos = [XGBRegressor(learning_rate=0.1, n_estimators=100, max_depth=3),\n",
    "                        SGDRegressor(random_state=SEED), \n",
    "                        LGBMRegressor(random_state=SEED, n_jobs=-1, force_row_wise=True, ),\n",
    "                        DecisionTreeRegressor(random_state=SEED),\n",
    "                        KNeighborsRegressor(n_neighbors=5,  weights='distance',n_jobs=-1),\n",
    "                        BaggingRegressor(),\n",
    "                        RandomForestRegressor(random_state=SEED)]\n",
    "\n",
    "    nome_dos_modelos = ['XGBoost', \n",
    "                        'SGD', \n",
    "                        'LGBM', \n",
    "                        'DecisionTree',\n",
    "                        'KNN',\n",
    "                        'Bagging',\n",
    "                        'RandomForest']\n",
    "\n",
    "    resultados0 = {}\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    for i in range(len(lista_de_modelos)):\n",
    "        print('Rodando modelo: ' + nome_dos_modelos[i])\n",
    "        \n",
    "        model = lista_de_modelos[i]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        print(y_pred)\n",
    "        \n",
    "        smape = 100 / len(y_test) * np.sum(2 * np.abs(np.expm1(y_pred) - np.expm1(y_test)) / (np.abs(np.expm1(y_test)) + np.abs(np.expm1(y_pred))))\n",
    "        # Calculo meigarom: 100 / len(y_test) * np.sum(2 * np.abs(y_pred - y_test) / (np.abs(y_test) + np.abs(y_pred)))\n",
    "\n",
    "        resultados0[nome_dos_modelos[i]] = [smape]\n",
    "    \n",
    "    resultados = pd.DataFrame(resultados0, index = nome_das_medidas).T\n",
    "\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation (model_name, X_training, Y_training, kfold, model, verbose = False ):\n",
    "    smape_list = []\n",
    "    \n",
    "    \n",
    "    # Star an end date validation\n",
    "    for k in reversed( range (1, kfold+1) ):\n",
    "        if verbose:\n",
    "            print('\\nKfold: {}'.format(k))\n",
    "        teste_size = (100/(k))/100\n",
    "        if teste_size < 1:\n",
    "            print(teste_size)\n",
    "            X_trein, X_test, y_trein, y_test = train_test_split(X_training, Y_training, test_size=teste_size, random_state=42)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # X_treino = X_training.loc[indices_amostra]\n",
    "        # print(f'X_treino: {X_treino}')\n",
    "        # Y_treino = Y_training.loc[indices_amostra]\n",
    "        # print(f'Y_treino: {Y_treino}')\n",
    "        \n",
    "        # X_test = X_training.drop(index=X_treino.index)\n",
    "        # print(f'X_test: {X_test}')\n",
    "        # Y_test = Y_training.drop(index=Y_treino.index)\n",
    "        # print(f'Y_test: {Y_test}')\n",
    "\n",
    "        model.fit(X_trein, y_trein)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        smape = 100 / len(y_test) * np.sum(2 * np.abs(np.expm1(y_pred) - np.expm1(y_test)) / (np.abs(np.expm1(y_test)) + np.abs(np.expm1(y_pred))))\n",
    "        # Calculo meigarom: 100 / len(y_test) * np.sum(2 * np.abs(y_pred - y_test) / (np.abs(y_test) + np.abs(y_pred)))\n",
    "        \n",
    "        # Guardando as performaces\n",
    "        smape_list.append( smape )\n",
    "        \n",
    "        \n",
    "    resposta_modelo = pd.DataFrame( {'Model Name': model_name,\n",
    "                                     'SMAPE': smape_list},\n",
    "                                   index = [0])\n",
    "    \n",
    "    return resposta_modelo #, curiosidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tunnung (X_train, y_train): \n",
    "    #RANDON SEARCH\n",
    "    param = {'n_estimators' : [1500, 1700, 3000, 3500],\n",
    "         'eta' : [0.01, 0.03],\n",
    "         'max_depth' : [3, 5, 9],\n",
    "         'subsample': [0.1, 0.5, 0.7],\n",
    "         'colsample_bytree' : [0.3, 0.7, 0.9],\n",
    "         'min_child_weight' : [3, 8, 15]}\n",
    "\n",
    "    MAX_EVAL = 10\n",
    "    \n",
    "    final_result = pd.DataFrame()\n",
    "    for i in range (MAX_EVAL):\n",
    "        # Escolher os parâmetros aleatórios\n",
    "        hp = { k: random.sample(v, 1)[0] for k, v in param.items() }\n",
    "        print(hp)\n",
    "\n",
    "\n",
    "        # model\n",
    "        model_xgb = XGBRegressor(objective = 'reg:squarederror',\n",
    "                                n_estimators=hp['n_estimators'],\n",
    "                                eta = hp['eta'],\n",
    "                                max_depth = hp['max_depth'],\n",
    "                                subsample = hp['subsample'],\n",
    "                                colsample_bytree = hp['colsample_bytree'],\n",
    "                                min_child_weight = hp['min_child_weight'])\n",
    "        # performance\n",
    "\n",
    "        result = cross_validation (model_name = 'XGBRegressor',\n",
    "                                   X_training = X_train,\n",
    "                                   Y_training = y_train,\n",
    "                                   kfold = 5,\n",
    "                                   model = model_xgb,\n",
    "                                   verbose = True )\n",
    "        final_result = pd.concat([final_result, result])\n",
    "    final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratamento_inicial (df):\n",
    "    \n",
    "    df = limpeza_inicial (df)\n",
    "    df_pd = expand_product_detais (df)\n",
    "    df = df_pd_plus_df(df, df_pd)\n",
    "    df = ajuste_colunas (df)\n",
    "    df = type_sex(df)\n",
    "    df = feature_engine (df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model(model, x_test):\n",
    "    \n",
    "    yhat = np.expm1(model.predict(x_test))\n",
    "    \n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Function Subission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(df_sub,yhat):\n",
    "    # Preparando os dados para a submissão\n",
    "\n",
    "    df_submission = pd.DataFrame()\n",
    "    df_submission['pid'] = df_sub['pid']\n",
    "    df_submission['actual_price'] = yhat\n",
    "\n",
    "    # Exportando como CSV\n",
    "    df_submission.to_csv('submissions/submission_g01.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_treino = '../2023-10-20-Hackday_6/dataset/raw/train.json'\n",
    "url_prod = '../2023-10-20-Hackday_6/dataset/raw/test.json'\n",
    "\n",
    "df_treino = load_data(url_treino)\n",
    "df_prod = load_data(url_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Run Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treino = tratamento_inicial(df_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prod = tratamento_inicial(df_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_df(df_treino)\n",
    "y_train = var_resposta(y_train)\n",
    "y_test = var_resposta(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prod = split_df(df_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = rescaling_train(X_train)\n",
    "X_test = rescaling_test(X_test)\n",
    "X_prod = rescaling_test(X_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tranform_train(X_train, y_train)\n",
    "X_test = tranform_test(X_test)\n",
    "X_prod = tranform_test(X_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = feature_selected_manual (X_train)\n",
    "X_test = feature_selected_manual (X_test)\n",
    "X_prod = feature_selected_manual (X_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetures = feature_select_LGBM (X_train, y_train)\n",
    "fetures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_selected_BORUTA(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = feature_selected_manual (X_train)\n",
    "X_test = feature_selected_manual (X_test)\n",
    "X_prod = feature_selected_manual (X_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = MachineLearning(X_train, y_train, X_test, y_test)\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tunnung (X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Submission Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDRegressor(random_state=42)\n",
    "model_train = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = best_model(model_train, X_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparando os dados para a submissão\n",
    "df_submission = pd.DataFrame()\n",
    "df_submission['pid'] = df_prod['pid']\n",
    "df_submission['actual_price'] = yhat\n",
    "# Exportando como CSV\n",
    "df_submission.to_csv('submissions/submission_g02.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('hd6': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "892e5ff5783333d733f0bd2e0068297de9409e96214f7e68af4469f351f76fae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
